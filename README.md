# Google Play Sentiment Analysis

A Pythonâ€‘based project to analyze user reviews from Google Play, determine customer sentiment, and build predictive models of satisfaction.

## ğŸ“– Overview

This project ingests and processes user reviews from Google Play to:
- **Understand user sentiment** via text analytics and rating distributions  
- **Identify key factors** driving satisfaction (e.g., review text, thumbsâ€‘up counts)  
- **Build classifiers** (Logistic Regression, Random Forest, Neural Network) to predict whether a review is â€œSatisfiedâ€ or â€œNot Satisfiedâ€ based on both numerical and textual features :contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}.

## ğŸ—‚ Dataset

- **Source:** Google Play user reviews (162 rows, 11 columns)  
- **Key columns:**  
  - `review_id`, `user_name`  
  - `rating` (1â€“5)  
  - `review_description` (text)  
  - `thumbs_up` (helpfulness count)  
  - `review_date`, `app_version`, `language_code`, `country_code`  
  - `developer_response`, `developer_response_date` :contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}.

## ğŸ”§ Data Preparation

1. **Loading & Exploration**  
   - Used Pandas to load CSV and inspect schema.  
2. **Cleaning**  
   - Filled missing `developer_response`/date with â€œNo responseâ€  
   - Imputed missing `app_version` by mode  
   - Dropped `review_title` (all null)  
   - Removed duplicate `review_description` entries :contentReference[oaicite:4]{index=4}&#8203;:contentReference[oaicite:5]{index=5}.  
3. **Type Conversion & Outliers**  
   - Converted dates to `datetime`; set invalid developerâ€‘response dates to `NaT`  
   - Removed outliers in `thumbs_up` via IQR method.  
4. **Feature Engineering**  
   - Minâ€“Max scaling for `rating` and `thumbs_up`  
   - Labelâ€encoding `app_version`; oneâ€‘hot for `language_code` and `country_code`  
   - TFâ€‘IDF vectorization (top 500 tokens) of `review_description` :contentReference[oaicite:6]{index=6}&#8203;:contentReference[oaicite:7]{index=7}.

## ğŸ“Š Exploratory Analysis

- **Ratings Distribution:** heavily skewed to 5â˜…, with few lowâ€rating outliers.  
- **Thumbsâ€‘Up:** most reviews have 0, a few highlyâ€liked outliers.  
- **App Versions:** concentrated on recent versions (e.g., â€œ18.0â€, â€œ17.0â€), reflecting active user feedback.  
- **Text Insights:** common positive words (`great`, `easy`, `love`), but also mentions of `issue`, `backup` :contentReference[oaicite:8]{index=8}&#8203;:contentReference[oaicite:9]{index=9}.

## ğŸ¤– Modeling

Three classification algorithms were evaluated:

| Model                | Key Settings                | Test Accuracy |
|----------------------|-----------------------------|--------------:|
| Logistic Regression  | max_iter=1000               |        96.67% |
| Random Forest        | n_estimators=100            |        96.67% |
| Neural Network       | 2 hidden dense layers, Dropout, 20Â epochs | 96.67% |

- **Features:** concatenated scaled numeric features + TFâ€‘IDF matrix  
- **Evaluation:** accuracy, precision, recall, F1â€‘score, crossâ€‘validation :contentReference[oaicite:10]{index=10}&#8203;:contentReference[oaicite:11]{index=11}.

## ğŸš€ Results & Insights

- **High performance:** All models achieved ~96.7% accuracy, showing ratings and review text strongly predict satisfaction.  
- **Feature importance:** Rating is the strongest predictor (corr.Â 0.86 with satisfaction) versus weak relation of thumbsâ€‘up (corr.Â 0.07).  
- **Version effects:** Certain app versions saw dips in satisfaction, suggesting targeted areas for improvement :contentReference[oaicite:12]{index=12}&#8203;:contentReference[oaicite:13]{index=13}.

## ğŸ”® Future Work

- Incorporate additional metadata (user profiles, review timestamps).  
- Experiment with advanced NLP (word embeddings, transformer models).  
- Deploy realâ€‘time analysis pipeline for live sentiment monitoring.  
- Address class imbalance and minority review patterns.
